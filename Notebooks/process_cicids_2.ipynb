{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d92648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from pathlib import Path\n",
    "import csv\n",
    "\n",
    "#  Define dataset folders\n",
    "\n",
    "DATA_DIR = Path(r\"D:/Dissertation 2025/Dataset\")\n",
    "OUT_DIR = Path(r\"D:/Dissertation 2025/Results\")\n",
    "\n",
    "files = {\n",
    "    \"Monday\": DATA_DIR / \"Monday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Tuesday\":   DATA_DIR / \"Tuesday-WorkingHours.pcap_ISCX.csv\",\n",
    "    \"Wednesday\": DATA_DIR / \"Wednesday-workingHours.pcap_ISCX.csv\",\n",
    "}\n",
    "\n",
    "print(\" Dataset folder:\", DATA_DIR)\n",
    "print(\" Output folder :\", OUT_DIR)\n",
    "\n",
    "\n",
    "#  Read ONLY the header (super safe)\n",
    "\n",
    "def safe_preview_header(path): \n",
    "    \"\"\"Reads only the first line (header). Never freezes.\"\"\"\n",
    "    with open(path,\n",
    "\"r\", encoding=\"latin-1\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        header = next(reader)\n",
    "        return header\n",
    "\n",
    "\n",
    "headers = {day: safe_preview_header(path) for day, path in files.items()\n",
    "}\n",
    "\n",
    "print(\"\\n=== Header Extraction Complete ===\")\n",
    "for day, hdr in headers.items():\n",
    "    print(f\"\\n{day} Columns ({len(hdr)}):\")\n",
    "    for h in hdr:\n",
    "        print(\" -\", h)\n",
    "\n",
    "\n",
    "# Detect leakage columns (IP, Port, Timestamp)\n",
    "\n",
    "def find_leakage_cols(header):\n",
    "    leak = []\n",
    "    for col in header:\n",
    "        c = col.lower()\n",
    "        if (\n",
    "            \"flow id\" in c or\n",
    "            \"src ip\" in c or \"dst ip\" in c or\n",
    "            \"source ip\" in c or \"destination ip\" in c or\n",
    "            \"port\" in c or\n",
    "            \"timestamp\" in c or\n",
    "            \"protocol\" in c\n",
    "        ):\n",
    "            leak.append(col)\n",
    "    return leak\n",
    "\n",
    "\n",
    "leakage_cols = {day: find_leakage_cols(h) for day, h in headers.items()\n",
    "}\n",
    "\n",
    "print(\"\\n=== Leakage Columns Detected ===\")\n",
    "for day, cols in leakage_cols.items():\n",
    "    print(day,\n",
    "\":\", cols)\n",
    "\n",
    "\n",
    "# Attack family mapping\n",
    "\n",
    "fam_map = {\n",
    "    \"FTP-PATATOR\": \"BruteForce\",\n",
    "    \"SSH-PATATOR\": \"BruteForce\",\n",
    "    \"DOS HULK\": \"DoS\",\n",
    "    \"DOS GOLDENEYE\": \"DoS\",\n",
    "    \"DOS SLOWLORIS\": \"DoS\",\n",
    "    \"WEB ATTACK â€“ SQL INJECTION\": \"Web\",\n",
    "    \"WEB ATTACK â€“ XSS\": \"Web\",\n",
    "    \"WEB ATTACK â€“ BRUTE FORCE\": \"Web\",\n",
    "    \"WEB ATTACK - SQL INJECTION\": \"Web\",\n",
    "    \"WEB ATTACK - XSS\": \"Web\",\n",
    "    \"WEB ATTACK - BRUTE FORCE\": \"Web\",\n",
    "    \"DDOS\": \"DDoS\",\n",
    "    \"PORTSCAN\": \"PortScan\",\n",
    "    \"BOT\": \"Botnet\",\n",
    "    \"INFILTRATION\": \"Infiltration\",\n",
    "}\n",
    "\n",
    "\n",
    "# Process each file in streaming mode\n",
    "\n",
    "\n",
    "def preprocess_day(day, path, header, leak_list, output_dir):\n",
    "    print(f\"\\nProcessing {day}: {path.name}\")\n",
    "\n",
    "    # Identify label column name\n",
    "    label_col = None\n",
    "    for col in header:\n",
    "        if col.lower() in {\n",
    "    \"label\",\n",
    "    \"attack\"\n",
    "}:\n",
    "            label_col = col\n",
    "            break\n",
    "\n",
    "    if not label_col:\n",
    "        raise ValueError(f\"No label column found in {path.name}\")\n",
    "\n",
    "    print(f\"ðŸ‘‰ Label column for {day} =\", label_col)\n",
    "\n",
    "    lf = pl.scan_csv(\n",
    "        path,\n",
    "        has_header=True,\n",
    "        infer_schema_length=0,\n",
    "        ignore_errors=True\n",
    "    )\n",
    "\n",
    "    # Drop leakage columns\n",
    "    for c in leak_list:\n",
    "        lf = lf.drop(c)\n",
    "\n",
    "    # Normalize label to uppercase & rename column\n",
    "    lf = lf.rename({label_col: \"Label\"\n",
    "})\n",
    "    lbl_up = pl.col(\"Label\").str.to_uppercase()\n",
    "\n",
    "    # Create y_binary\n",
    "    y_binary = (lbl_up != \"BENIGN\").cast(pl.Int8).alias(\"y_binary\")\n",
    "\n",
    "    # y_family using dictionary mapping\n",
    "    y_family = (\n",
    "        pl.col(\"Label\")\n",
    "        .replace(fam_map)\n",
    "        .fill_null(\"OtherAttack\")\n",
    "        .alias(\"y_family\")\n",
    "    )\n",
    "\n",
    "    # Add columns\n",
    "    lf = lf.with_columns([\n",
    "        lbl_up.alias(\"Label\"),\n",
    "        y_binary,\n",
    "        y_family,\n",
    "        pl.lit(day).alias(\"day\")\n",
    "])\n",
    "\n",
    "    # Output file\n",
    "    out_path = output_dir / f\"{day}_clean.csv\"\n",
    "    print(f\"Saving cleaned file to: {out_path}\")\n",
    "\n",
    "    # Stream results to CSV (no RAM use)\n",
    "    lf.sink_csv(out_path)\n",
    "\n",
    "    print(f\"Finished {day}\")\n",
    "    return out_path\n",
    "\n",
    "\n",
    "# Run preprocessing for each day\n",
    "output_paths = {}\n",
    "\n",
    "for day, path in files.items():\n",
    "    output_paths[day\n",
    "] = preprocess_day(\n",
    "        day=day,\n",
    "        path=path,\n",
    "        header=headers[day\n",
    "],\n",
    "        leak_list=leakage_cols[day\n",
    "],\n",
    "        output_dir=OUT_DIR\n",
    "    )\n",
    "\n",
    "print(\"\\nCleaned files saved:\")\n",
    "for day, p in output_paths.items():\n",
    "    print(day,\n",
    "\"â†’\", p)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c1c590",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
